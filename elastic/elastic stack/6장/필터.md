- [필터](#필터)
  - [문자열 자르기](#문자열-자르기)
    - [mutate](#mutate)
    - [dissect](#dissect)
    - [grok](#grok)
  - [대소문자 변경](#대소문자-변경)
  - [날짜/시간 문자열 분석](#날짜시간-문자열-분석)
  - [조건문](#조건문)

---
# 필터

- 로그스태시에서 필터는 입력 플러그인이 받은 데이터를 의미 있는 데이터로 구조화하는 역할을 한다.
- 필수 구성 요소는 아니다.
  - 그러나 필터 없는 파이프라인은 그 기능을 온전히 발휘하기가 힘들다.
- **로그스태시 필터는 비정형 데이터를 정형화하고, 데이터 분석을 위한 구조를 잡아준다.**
![](/images/2022-04-14-02-03-40.png)
- 비츠나 키바나 등에서 입력받은 데이터를 로그스태시 필터를 이용해 필요한 정보만 손쉽게 추출하거나 형태를 변환하고 부족한 정보는 추가하는 등 전반적인 데이터 정제/가공 작업을 쉽게 수행할 수 있다.
- 이렇게 정형화된 데이터는 엘라스틱서치나 아마존 S3같은 스토리지에 전송되어 분석 등의 용도로 활용된다.
- 로그스태시, 그 중에서 필터는 데이터를 정형화하고 사용자가 필요한 데이터 형태로 가공하는 데 핵심적인 역할을 한다.
- 필터 역시 플러그인 형태이며 입력과 비슷하게 다양한 필터 플러그인이 존재한다.

> 자주 사용되는 필터 플러그인

| 필터 플러그인 | 설명                                                                                                                                                                          |
| ------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| grok          | grok 패턴을 사용해 메시지를 구조화된 형태로 분석한다. grok 패턴은 일반적인 정규식과 유사하나, 추가적으로 미리 정의된 패턴이나 필드 이름 설정, 데이터 타입 정의 등을 도와준다. |
| dissect       | 간단한 패턴을 사용해 메시지를 구조화된 형태로 분석한다. 정규식을 사용하지 않아 grok에 비해 자유도는 조금 떨어지지만 더 빠른 처리가 가능하다                                   |
| mutate        | 필드명을 변경하거나 문자열 처리 등 일반적인 가공 함수들을 제공한다                                                                                                            |
| date          | 문자열을 지정한 패턴의 날짜형으로 분석한다                                                                                                                                    |



> 예제 실습하기

우선 예제를 진행하기 위한 파일을 하나 만들어본다.

```txt
[2020-01-02 14:17] [ID1] 192.10.2.6 9500 [INFO] - connected.
[2020/01/02 14:19:25] [ID2] 218.25.32.70 1070 [warn] - busy server.
```

나는 `/Users/user/dev/logstash-7.10.0/sample-data` 경로에 `filter-example.log` 라는 이름으로 파일을 만들었다.

그리고 `logstash-test.config` 파일을 다음과 같이 수정하자.
```
input {
  file {
    path => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

output {
  stdout { }
}
```
- 방금 만든 `filter-example.log` 파일을 입력으로 받는 파이프라인 설정이다.
- `start_position`은 새로운 파일을 인식했을 때, 파일을 어디서부터 읽을 것인지에 대한 옵션으로, `beginning`은 파일의 처음, `end`는 파일의 끝을 가리킨다.
- 파일 플러그인에 `sincedb_path`라는 옵션이 추가되었다.
  - 우리는 로그 스태시를 실행할 때마다 `filter-example.log` 파일의 처음부터 시작해야 한다.
  - `nul` 로 지정하면, `sincedb` 파일을 만들지 않기 때문에 이전 파일을 읽었던 기록이 없어서 매번 로그 스태시를 실행할 때마다 파일을 처음부터 읽게 된다. (아, 근데 `nul`설정은 윈도우 한정임, 그래서 mac인 사람들은 /dev/null 로 변경한 뒤 sudo 명령어로 실행하거나, `sincedb_clean_after => 0` 을 pipeline input file에 넣어주면 동일한 효과를 낸다. [참고](https://github.com/logstash-plugins/logstash-input-file/issues/275#issuecomment-714608218), 근데 계속 sudo로 실행해야 한다는 점은 좀 불편하다 ;))

> 아무튼 맥 버전은 다음과 같이 세팅을 하면 된다.
```
input {
  file {
    path => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    sincedb_clean_after => 0
  }
}

filter {
  mutate {
    split => { "message" => " " }
  }
}

output {
  stdout { }
}
```
- 근데 **sincedb 데이터베이스 파일**은 무엇일까?
  - sincedb 데이터 베이스 파일은 **파일을 어디까지 읽었는지 기록하는 파일**이다.
  - 앞에서 `start_position`이 파일을 어디서부터 읽을지 설정한다고 했다.
  - `beginning`은 파일의 처음부터 읽어오고, `end`는 파일의 끝에서부터 읽어온다.
  - 특이한 것은 `start_position`은 파일을 불러들이는 최초에 한 번만 적용된다는 점이다.
  - 만약 sincedb_path 옵션을 지우고 로그스태시를 실행해보면 처음엔 로그가 뜨지만, 로그 스태시를 종료했다가 다시 실행시키면 로그가 보이지 않는다.
    -  (또한, 실행 로그를 읽어보면 sincedb set을 하지 않았다는 로그가 보인다.
    -  `No sincedb_path set, generating one based on the "path" setting {:sincedb_path=>"/Users/user/dev/logstash-7.10.0/data/plugins/inputs/file/.sincedb_7e4cfec86e09945fa8d60342cbeebf70`
    -  최초 실행될 때는 `start_position` 옵션에 맞춰 파일을 읽는다. 즉 파일의 처음부터 2줄을 읽는다.
    -  그리고 sincedb 데이터 베이스 파일에 어디까지 읽었는지 기록이 된다.
    -  이후에 로그 스태시를 재실행하면, start_position을 보고 파일을 불러오는 것이 아니라, sincedb 데이터베이스 파일이 있는지 보고, 있다면 sincedb 데이터베이스 파일에 기록되어 있는 위치부터 파일을 읽는다.
    -  sincedb_path를 따로 입력하지 않으면 엘라스틱은 기본값으로 로그스태시 data 폴더의 data/plugins/inputs/file 위치에 sincedb 데이터베이스 파일을 생성한다.
    -  파일이 있는 경로로 이동한 다음에 sincedb 파일을 확인해보자
    -  ![](/images/2022-04-14-02-59-07.png)
    -  파일을 열어보면, 파일이 마지막으로 읽힌 오프셋값을 알 수 있다.

- sincedb_path를 파이프라인에 적용한 상태에서 로그스태시를 실행시켜보자.
```sh
➜ logstash-7.10.0 bin/logstash -f config/logstash-test.conf

...
{
       "message" => "[2020-01-02 14:17] [ID1] 192.10.2.6 9500 [INFO] - connected.",
    "@timestamp" => 2022-04-13T19:35:51.774Z,
      "@version" => "1",
          "host" => "AD01718275.local",
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
}
{
       "message" => "[2020/01/02 14:19:25] [ID2] 218.25.32.70 1070 [warn] - busy server.",
    "@timestamp" => 2022-04-13T19:35:51.785Z,
      "@version" => "1",
          "host" => "AD01718275.local",
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
}
```

## 문자열 자르기

### mutate
- 데이터나 로그는 대부분 길이가 길기에, 우리가 원하는 형태로 분리해야 한다.
- 먼저 `logstash-test.conf` 파일에서 filter에 플러그인을 추가해보자

```
input {
  file {
    path => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
    start_position => "beginning"
    sincedb_path => "nul"
  }
}

filter {
  mutate {
    split => { "message" => " " }
  }
}

output {
  stdout { }
}
```

- mutate plugin은 필드를 변형하는 다양한 기능들을 제공한다.
  - 필드 이름을 바꾸거나, 변경하거나, 삭제하는 작업 등을 할 수 있다.
  - `mutate`는 플러그인 내부에 옵션이 다양한데, `split`도 여러 옵션 중 하나이다.
  - `split` 옵션은 구분자를 기준으로 데이터를 자를 수 있다.
  - 위 예시는 `message`라는 필드를 공백 기준(" ")으로 문자를 분리한다.


> mutate 옵션

| mutate 옵션 | 설명                                                           |
| ----------- | -------------------------------------------------------------- |
| split       | 쉼표(,) 같은 구분 문자를 기준으로 문자열을 배열로 나눈다.      |
| rename      | 필드 이름을 바꾼다.                                            |
| replace     | 해당 필드 값을 특정 값으로 바꾼다.                             |
| uppercase   | 문자를 대문자로 변경한다.                                      |
| lowercase   | 문자를 소문자로 변경한다.                                      |
| join        | 배열을 쉼표(,) 같은 구분 문자로 연결해 하나의 문자열로 합친다. |
| gsub        | 정규식이 일치하는 항목을 다른 문자열로 대체한다.               |
| merge       | 특정 필드를 다른 필드에 포함시킨다.                            |
| coerce      | null인 필드값에 기본 값을 넣어준다.                            |
| strip       | 필드값의 좌우 공백을 제거한다.                                 |

**mutate는 많은 옵션이 있어서 순서가 중요하다.**

`coerce`- `rename`- `update`- `replace` - `convert` - `gsub` - `uppercase` - `capitalize` - `lowercase` - `strip`- `remove` - `split` - `join` - `merge` -`copy` 순으로 적용된다.


<br/>

아무튼, filter를 적용한 로그스태시를 실행시켜보자.

```sh
sudo bin/logstash -f config/logstash-test.conf
```

```json
{
    "@timestamp" => 2022-04-13T20:04:15.973Z,
          "host" => "AD01718275.local",
      "@version" => "1",
       "message" => [
        [0] "[2020-01-02",
        [1] "14:17]",
        [2] "[ID1]",
        [3] "192.10.2.6",
        [4] "9500",
        [5] "[INFO]",
        [6] "-",
        [7] "connected."
    ],
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
}
{
    "@timestamp" => 2022-04-13T20:04:15.993Z,
          "host" => "AD01718275.local",
      "@version" => "1",
       "message" => [
        [0] "[2020/01/02",
        [1] "14:19:25]",
        [2] "[ID2]",
        [3] "218.25.32.70",
        [4] "1070",
        [5] "[warn]",
        [6] "-",
        [7] "busy",
        [8] "server."
    ],
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
}
```

- message 필드 문자열이 공백을 기준으로 구분되어 배열 형태의 데이터가 되었다.
- 구분된 문자들은 `필드명[숫자]` 와 같이 접근할 수 있다.
- 예를 들어 ID를 가리키는 필드는 `message[2]`와 같다.



> 여담이지만,,,,,,

***sudo 써서 실행하는거 귀찮아 죽겠습니다.***
```
  Error: Permission denied - Permission denied
  Exception: Errno::EACCES
  Stack: org/jruby/RubyFile.java:1267:in `utime'
```
*이 에러는 다음과 같이 sincedb_file을 바꿔주면 된다고 하는군요.*
```
input {
  file {
    path => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
    start_position => "beginning"
    sincedb_path => "/tmp/mysincedbfile"
    sincedb_clean_after => 0
 }
}

...
```
- [Reference](https://github.com/elastic/logstash/issues/3311#issuecomment-113090148) -> 이제 `sudo` 없이 실행에 문제 없습니다! 

### dissect

- mutate 플러그인의 split 옵션은 하나의 구분자만 이용해서 데이터를 나눠야 한다는 단점이 있다.
- dissect 플러그인은 **패턴을 이용해 문자열을 분석하고, 주요 정보를 필드로 추출하는 기능**을 수행한다.

logstash-test.conf를 다음과 같이 수정한다
```
input {
  file {
    path => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    sincedb_clean_after => 0
  }
}

filter {
  dissect {
    mapping => {"message" => "[%{timestamp}] [%{id}] %{ip} %{port} [%{level}] - %{message}."}
  }
}

output {
  stdout { }
}
```

- `filter-example.log`는 다음과 같다.
```
[2020-01-02 14:17] [ID1] 192.10.2.6 9500 [INFO] - connected.
[2020/01/02 14:19:25]   [ID2] 218.25.32.70 1070 [warn] - busy server.
```



- 실행
```sh
sudo ./bin/logstash -f config/logstash-test.conf
```
- 실행 결과
```sh
{
            "id" => "ID1",
      "@version" => "1",
    "@timestamp" => 2022-04-15T14:37:20.241Z,
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log",
     "timestamp" => "2020-01-02 14:17",
            "ip" => "192.10.2.6",
          "host" => "Dohas-MacBook-Pro.local",
          "port" => "9500",
         "level" => "INFO",
       "message" => "connected"
}
{
      "@version" => "1",
    "@timestamp" => 2022-04-15T14:37:20.242Z,
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log",
          "tags" => [
        [0] "_dissectfailure"
    ],
          "host" => "Dohas-MacBook-Pro.local",
       "message" => "[2020/01/02 14:19:25]   [ID2] 218.25.32.70 1070 [warn] - busy server."
}
```

> 개념 정리
- dissect 플러그인의 mapping 옵션에 구분자 형태를 정의하고 필드를 구분한다.
- `%{필드명}`으로 작성하면, 중괄호({}) 안의 필드명으로 새로운 필드가 만들어진다.
- `%{}` 외의 문자들은 모두 구분자 역할을 한다.
- 현재 이 예제에서 필드를 구분하는 구분자로는 공백, -, [, ] 가 있다.
- 첫 번째 결과를 보면 mutate 플러그인보다 훨씬 깔끔하게 파싱된 것을 확인할 수 있다.
- 그러나 두 번째 로그에서 문제가 발생한다.
  - `_dissectfailure` 는 dissect 필터 플러그인이 동작하지 않을 경우 발생한다.
  - `dissect` 플러그인의 매핑 패턴은 [timestamp]와 [id] 간의 공백이 한 칸이다.
  - 공백이 한 칸인 첫 번째 로그는 패턴에 맞춰 문자가 잘 구분되었으나, 두 번째 로그는 [timestamp]와 [id]간의 공백이 세 칸이기 때문에 매핑에서 정해놓은 구분자가 아니라며 오류를 발생시킨다.
  - dissect 플러그인에서는 공백 한 칸과 세 칸을 다르게 인식하기 때문이다.

> dissect 플러그인의 공백 문제를 해결할 수 있는 방법

logstash-test.conf 를 다음과 같이 수정해본다.
```
input {
  file {
    path => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    sincedb_clean_after => 0
  }
}

filter {
  dissect {
    mapping => {"message" => "[%{timestamp}]%{?->} [%{id}] %{ip} %{+ip} [%{?level}] - %{}."}
  }
}

output {
  stdout { }
}
```
- `->` : 공백을 무시한다.
- `%{필드명->}` : 공백이 몇 칸이든 하나의 공백으로 인식
- `%{?필드명}`, `%{}` : 그 필드명은 결과에 포함하지 않는다.
- `%{?->}` : 공백들을 하나의 필드로 만든 다음 결과엔 포함하지 않는다.
  - 위의 예시에서는 기존에 있던 공백들이나, level이나, message 필드를 모두 무시할 것이다.
- `%{+필드명}` : 여러 개의 필드를 하나의 필드로 합쳐서 표현
  - 위의 예시에서는 기존 port 필드가 ip 필드에 합쳐지게 된다.

<br/>

- 그리고 다시 로그스태시를 실행시켜보자.
```sh
sudo bin/logstash -f config/logstash-test.conf
```

- 그 결과는 다음과 같다
```
{
       "message" => "[2020/01/02 14:19:25]   [ID2] 218.25.32.70 1070 [warn] - busy server.",
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log",
            "ip" => "218.25.32.70 1070",
      "@version" => "1",
          "host" => "AD01718275.local",
     "timestamp" => "2020/01/02 14:19:25",
            "id" => "ID2",
    "@timestamp" => 2022-04-16T11:44:18.577Z
}
{
       "message" => "[2020-01-02 14:17] [ID1] 192.10.2.6 9500 [INFO] - connected.",
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log",
            "ip" => "192.10.2.6 9500",
      "@version" => "1",
          "host" => "AD01718275.local",
     "timestamp" => "2020-01-02 14:17",
            "id" => "ID1",
    "@timestamp" => 2022-04-16T11:44:18.555Z
}
```

### grok

- `grok`는 정규 표현식을 이용해 문자열을 파싱할 수 있다.
  - 정규 표현식은 특정한 규칙을 갖는 문자열을 표현하는 언어이다.
- `grok`는 자주 사용하는 정규 표현식들을 패턴화해뒀으며, 패턴을 이용해 **`%{패턴: 필드명}`** 형태로 데이터에서 특정 필드를 파싱할 수 있다.

> 자주 사용되는 grok 패턴

| 패턴명            | 설명                                                                                                                                                                                       |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| NUMBER            | 10진수를 인식한다. 부호와 소숫점을 포함할 수 있다.                                                                                                                                         |
| SPACE             | 스페이스, 탭 등 하나 이상의 공백을 인식한다.                                                                                                                                               |
| URI               | URI를 인식한다. 프로토콜, 인증 정보, 호스트, 경로, 파라미터를 포함할 수 있다.                                                                                                              |
| IP                | IP 주소를 인식한다. IPv4나 IPv6 모두 인식 가능하다.                                                                                                                                        |
| SYSLOGBASE        | 시스로그의 일반적인 포맷에서 타임스탬프, 중요도, 호스트, 프로세스 정보까지 메시지 외의 헤더 부분을 인식한다.                                                                               |
| TIMESTAMP_ISO8601 | ISO8601 포맷의 타임스탬프를 인식한다. 2022-04-17T12:00:00+09:00 과 같은 형태이며, 타임존까지 정확한 정보를 기록하고 로그에선 많이 쓰는 날짜 포맷이기에 grok 표현식을 작성할 때도 유용하다. |
| DATA              | 이 패턴의 직전 패턴부터 다음 패턴 사이를 모두 인식한다. 특별히 인식하고자 하는 값의 유형을 신경 쓸 필요가 없으므로, 특별히 값이 검증될 필요가 없다면 가장 많이 쓰는 패턴 중 하나이다.      |
| GREEDYDATA        | DATA 타입과 동일하나, 표현식의 가장 뒤에 위치시킬 경우 해당 위치부터 이벤트의 끝까지를 값으로 인식한다.                                                                                    |


- logstash-test.conf 파일을 다음과 같이 수정해보자.

```
input {
  file {
    path => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    sincedb_clean_after => 0
  }
}

filter {
  grok {
    match => { "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] [ ]*\[%{DATA:id}\] %{IP:ip} %{NUMBER:port:int} \[%{LOGLEVEL:level}\] \- %{DATA:msg}\."}
  }
}

output {
  stdout { }
}
```

- `grok`은 기본적으로 `%{패턴명:변수명}` 형태로 작성하면 된다.
- 먼저, `TIMESTAMP_ISO8601` 은 ISO8601 표준 시간 표기법에 대한 패턴이다.
- DATA는 모든 데이터를 인식한다.
- IP는 IPv4 형태의 데이터를 인식한다.
- **NUMBER는 숫자를 인식하는데, 변수명 뒤에 `:int` 를 추가하면 변경 시 정수 타입으로 지정한다. 특별한 값을 넣지 않으면 모든 데이터가 문자 타입으로 인식된다.**
- LOGLEVEL은 시스로그(syslog) 레벨(WARN, ERROR 등)을 인식한다.
- `[`, `]`, `-`, `.` 같은 기호는 역슬래시(\\)를 붙여서 이스케이프 할 수 있다.
- [timestamp]와 [id] 사이 공백이 한 칸인 것과 세칸인 경우가 있는데 `[ ]*` 라는 정규식을 이용하여 모든 공백을 허용했다.
- 이제, 로그 스태시를 실행해보자.
```json
{
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log",
       "message" => "[2020/01/02 14:19:25]   [ID2] 218.25.32.70 1070 [warn] - busy server.",
      "@version" => "1",
          "tags" => [
        [0] "_grokparsefailure"
    ],
          "host" => "AD01718275.local",
    "@timestamp" => 2022-04-16T18:46:28.026Z
}
{
      "@version" => "1",
       "message" => "[2020-01-02 14:17] [ID1] 192.10.2.6 9500 [INFO] - connected.",
    "@timestamp" => 2022-04-16T18:46:28.009Z,
           "msg" => "connected",
          "host" => "AD01718275.local",
            "id" => "ID1",
     "timestamp" => "2020-01-02 14:17",
            "ip" => "192.10.2.6",
          "port" => 9500,
         "level" => "INFO",
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
}
```

- 첫 번째 로그(ID1)의 경우 원하는 형태로 필드가 구분이 됐고, 포트가 문자 타입이 아닌 정수 타입으로 저장됐다.
- 두 번째 로그(ID2)의 경우 `날짜/시간` 데이터 포맷이 맞지 않아서 오류가 발생했다.(연-월-일 이 아니라, 연/월/일 로 되어 있다)
  - 로그를 만드는 쪽에선 너무나도 다양한 형태로 포맷을 만들기 때문에 데이터를 수집하는 쪽에서 반드시 포맷을 통일해야 한다.
  - 현재 제공하고 있는 grok 패턴으로는 두 번째 로그의 날짜/시간을 인식할 수 없기 때문에 새로운 패턴을 만들어볼 것이다.

> 로그스태시에서 지원하는 grok TIMESTAMP_ISO8601 패턴

```
TIMESTAMP_ISO8601 %{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?
```
- 연-월-일 형태의 포맷만 지원하고 있는데, 연/월/일 형태의 포맷도 지원되도록 수정해보자.
- 사용자가 패턴을 지정하기 위해서는 grok의 `pattern_definitions` 옵션을 사용한다.
- 여기선 TIMESTAMP_ISO8601 포맷을 기초로 기호만 추가해보려 한다.

```
input {
  file {
    path => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    sincedb_clean_after => 0
  }
}

filter {
  grok {
    pattern_definitions => { "MY_TIMESTAMP" => "%{YEAR}[/-]%{MONTHNUM}[/-]%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?" }
    match => { "message" => "\[%{MY_TIMESTAMP:timestamp}\] [ ]*\[%{DATA:id}\] %{IP:ip} %{NUMBER:port:int} \[%{LOGLEVEL:level}\] \- %{DATA:msg}\."}
  }
}

output {
  stdout { }
}
```

- grok 플러그인에서 `pattern_definitions` 옵션을 추가하고 원하는 형식의 정규표현식을 작성하면 된다.
- 기존 TIMESTAMP_ISO8601 포맷은 `연-월-일` 형태만 지원했는데, 이 패턴을 복서해서 `연/월/일` 형태까지 지원하도록 추가한 것이다.
- 정규표현식에서 `[/-]`는 `/`, `-` 기호 모두 패턴에 맞는다고 판단한다.
- 결과를 확인해보자. 두 로그 모두 정상적으로 파싱이 되었다.
```json
{
       "message" => "[2020-01-02 14:17] [ID1] 192.10.2.6 9500 [INFO] - connected.",
          "port" => 9500,
           "msg" => "connected",
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log",
         "level" => "INFO",
      "@version" => "1",
          "host" => "AD01718275.local",
            "id" => "ID1",
    "@timestamp" => 2022-04-16T19:06:01.124Z,
     "timestamp" => "2020-01-02 14:17",
            "ip" => "192.10.2.6"
}
{
       "message" => "[2020/01/02 14:19:25]   [ID2] 218.25.32.70 1070 [warn] - busy server.",
          "port" => 1070,
           "msg" => "busy server",
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log",
         "level" => "warn",
      "@version" => "1",
          "host" => "AD01718275.local",
            "id" => "ID2",
    "@timestamp" => 2022-04-16T19:06:01.136Z,
     "timestamp" => "2020/01/02 14:19:25",
            "ip" => "218.25.32.70"
}
```


- `dissect`와 `grok` 플러그인은 패턴을 이용해 구문 분석을 한다는 공통점이 있지만, 성능 차이가 있다.
- 로그 형식이 일정하고 패턴이 변하지 않는다면 `dissect`를 이용하라.
  - 패턴을 분석하지 않아서 속도가 빠르기 때문이다.
- 로그 형태가 일정하다고 장담하기 어렵다면, 예외 처리나 패턴이 자유로운 grok을 사용해라.
  - `dissect`로 분석이 가능한 문자열은 무조건 dissect를 쓰는 것이 성능상 좋고, grok의 경우 dissect와 기타 필터 조합으로 간단하게 해결되지 않는 진짜 정규 표현식이 필요한 경우에만 사용하는 것이 좋다.

## 대소문자 변경
- 로그스태시의 장점이 플러그인을 통해 문자열을 원하는 형태로 쉽게 가공할 수 있다는 것을 알고 있다.
- 이번엔 소문자를 대문자로 바꿔보면서 필터 플러그인의 파워를 느껴보자.

logstash-test.conf 파일을 다음과 같이 수정해본다.
```
input {
  file {
    path => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    sincedb_clean_after => 0
  }
}

filter {
  dissect {
    mapping => {"message" => "[%{?timestamp}]%{?->}[%{?id}] %{?ip} %{?port} [%{level}] - %{?msg}."}
  }
  mutate {
    uppercase => ["level"]
  }
}

output {
  stdout { }
}
```
- `level` 필드를 제외하고 나머지는 `?`를 사용해 모두 무시한다.
- 다음으로 mutate 플러그인의 uppercase 옵션을 통해 level 필드의 데이터를 모두 대문자로 변경한다.
- 로그 스태시를 실행해보고 결과를 확인해보자
```
{
          "host" => "AD01718275.local",
    "@timestamp" => 2022-04-17T08:23:08.696Z,
         "level" => "WARN",
       "message" => "[2020/01/02 14:19:25]   [ID2] 218.25.32.70 1070 [warn] - busy server.",
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log",
      "@version" => "1"
}
{
          "host" => "AD01718275.local",
    "@timestamp" => 2022-04-17T08:23:08.679Z,
         "level" => "INFO",
       "message" => "[2020-01-02 14:17] [ID1] 192.10.2.6 9500 [INFO] - connected.",
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log",
      "@version" => "1"
}
```
- `warn` 이 `WARN`으로 변경되었음을 확인할 수 있다.
- 반대로 lowercase 라는 옵션도 있다.



## 날짜/시간 문자열 분석
- 이벤트가 발생한 날짜/시간 정보는 모니터링이나 진단을 하는 데 없어서는 안되는 매우 중요한 정보다.
- 하지만 로그 생성자들이 만드는 날짜/시간 포맷이 통일되어 있지 않다는 문제가 있다.
- 예를 들어 12시 25분, 12:45, PM 12:45와 같이 다양하게 표현 가능하다.
- 이런 다양한 포맷을 `date 플러그인`을 이용해 기본 날짜/시간 포맷으로 인덱싱할 수 있다.
- 엘라스틱서치의 경우 ISO8601 표준 포맷(yyyy-MM-dd\`T\`HH:mm:ss.SSSSSSZ, yyyy-MM-dd, epoch_millis)을 기본으로 사용하고 있다.

<br/>

- 이번에는 date 플러그인을 사용해보기 위해 logstash-test.conf를 다음과 같이 수정해보자

```
input {
  file {
    path => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    sincedb_clean_after => 0
  }
}

filter {
  dissect {
    mapping => {"message" => "[%{timestamp}]%{?->}[%{?id}] %{?ip} %{?port} [%{?level}] - %{?msg}."}
  }
  mutate {
    strip => "timestamp"
  }
  date {
    match => [ "timestamp", "YYYY-MM-dd HH:mm", "yyyy/MM/dd HH:mm:ss" ]
    target => "new_timestamp"
    timezone => "UTC"
  }
}

output {
  stdout { }
}
```
필터부터 차근차근 살펴보자.
- 먼저, dissect 를 이용하여 문자열을 하나하나 자른다.
  - 이 때, ?가 붙어있는 나머지 필드들은 무시하고, timestamp만 남겨놨다.
- 다음 mutate 플러그인을 이용하여 strip 옵션으로 timestamp를 태운다.
  - strip 옵션은 선택한 필드의 양 옆에 공백이 있을 경우 제거하는데, dissect에 의해 만들어진 timestamp 필드 좌우에 공백이 포함되어 있다면 제거가 된다.
- **date 플러그인은 날짜/시간 관련된 데이터를 ISO8601 타입으로 일괄적으로 변경한다.**
  - `match`의 첫번째 값은 매칭할 필드명이고, 이후 값은 매칭할 날짜/시간 포맷이다.
  - 여기서는 timestamp 필드 중에서 `YYYY-MM-dd HH:mm` 포맷이거나 `YYYY/MM/dd HH:mm:ss` 포맷인 경우 매칭이 된다는 뜻이다.
  - target은 match에 의해 매칭된 필드가 새로 저장될 새로운 필드를 의미한다.
  - timezone은 원본 문자열에 타임존 정보가 포함되어 있지 않을 때 어떤 타임존으로 분석할 지 설정할 수 있다.
  - **로그스태시는 타임존을 지정하지 않으면 로컬PC의 타임존을 적용하게 된다**
    - 이 경우 표준시간대(UTC: Universal Time Coordinated)와 맞지 않을 수 있으니 타임존을 UTC로 설정하자.
  - 로그 스태시에서 사용하는 날짜/시간 포맷은 [Joda Time 라이브러리](https://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat.html) 포맷을 사용한다.

> Joda Time 라이브러리에서 지원하는 날짜/시간 패턴 문자열

| 기호 | 의미          | 예시          |
| ---- | ------------- | ------------- |
| Y/y  | 연도          | 2022          |
| M    | 월            | July, Jul, 07 |
| m    | 분            | 30            |
| D    | 일(연 기준)   | 189           |
| d    | 일(월 기준)   | 10            |
| H    | 시(0~23)      | 18            |
| h    | 시(0~12)      | 6             |
| S    | 밀리초(0~999) | 975           |
| s    | 초(0~59)      | 55            |

- 이제 로그스태시를 실행해보자.
```sh
bin/logstash -f config/logstash-test.conf
```

![](/images/2022-04-17-17-44-36.png)

- `new_timestamp`라는 새로운 필드가 생성되었고, 시간 포맷이 달랐던 두 데이터가 ISO8601 타입의 포맷으로 통일되는 것을 확인할 수 있다.

## 조건문

- 필터는 기본적으로 모든 이벤트에 적용된다.
- 단순히 csv나 json과 같이 정해진 포맷을 읽는 경우가 아니라, 일반적으로 입력되는 이벤트 형태는 가지각색이니, 각기 다른 필터를 적용해야 할 것이다.
- 로그스태시에서는 if, else if, else 조건문을 제공하며, 이를 이용해 이벤트마다 적절한 필터를 적용할 수 있다.

한 번 조건문을 사용해보기위해 logstash-test.conf를 다음과 같이 수정해보자.

```
input {
  file {
    path => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    sincedb_clean_after => 0
  }
}

filter {
  dissect {
    mapping => {"message" => "[%{timestamp}]%{?->}[%{id}] %{ip} %{port} [%{level}] - %{msg}."}
  }
  if [level] == "INFO" {
    drop { }
  }
  else if [level] == "warn" {
    mutate {
      remove_field => [ "ip", "port", "timestamp", "level" ]
    }
  }
}

output {
  stdout { }
}
```

- 필터 내부에서 조건문을 사용할 수 있다.
- dissect 플러그인을 통해 먼저 timestamp, id, ip, port, level, msg 필드를 생성한다.
- 다음으로 if 조건문을 통해 필드명이 특정 조건과 일치하는지 확인하는데, level은 앞서 dissect에서 만들어진 필드다.
- 파이프라인 순서대로 동작하기 때문에 앞에서 level 필드가 만들어져야만 사용할 수 있다.
- `drop`은 **데이터를 삭제**하는 플러그인이다.
- 조건문과 결합하면, 특정 조건을 만족하는 로그를 버릴 수 있다.
- 여기서는 level 필드의 값에 따라서 로그를 삭제하거나 특정 필드를 제거하고 있다.

- 로그 스태시를 실행하면 다음과 같다.
```
{
          "path" => "/Users/user/dev/logstash-7.10.0/sample-data/filter-example.log",
    "@timestamp" => 2022-04-17T08:48:27.242Z,
          "host" => "AD01718275.local",
            "id" => "ID2",
      "@version" => "1",
           "msg" => "busy server",
       "message" => "[2020/01/02 14:19:25]   [ID2] 218.25.32.70 1070 [warn] - busy server."
}
```

- 조건문에 의해 level이 INFO 였던 첫 번째 로그는 출력되지 않았다.
- 그리고 level이 `warn`이었던 두 번째 로그는 mutate 플러그인에 의해 ip, port, timestamp, level 필드가 사라진 것을 확인할 수 있다.
