# 분석기
- [분석기](#분석기)
  - [분석기 구성](#분석기-구성)
    - [역인덱싱](#역인덱싱)
    - [분석기 API](#분석기-api)
    - [분석기 종류](#분석기-종류)

- 엘라스틱 서치는 전문 검색을 지원하기 위해 `역인덱싱 기술`을 사용한다.
- **전문 검색이란, 장문의 문자열에서 부분 검색을 수행하는 것이며, 역인덱싱은 장문의 문자열을 분석해 작은 단위로 쪼개어 인덱싱하는 기술이다.**
- 역인덱싱을 이용한 전문 검색에서는 양질의 결과를 얻기 위해 `문자열을 나누는 기준`이 중요하며, 이를 지원하기 위해 엘라스틱 서치에서는 `캐릭터 필터, 토크나이저, 토큰 필터`로 구성되어 있는 분석기 모듈을 갖고 있다.
- 분석기에는 하나의 토크나이저가 반드시 포함되어야하며, 캐릭터 필터와 토큰 필터는 옵션이다.

<br/>


> #### 토큰과 용어
- 앞으로 토큰(token)과 용어(term)라는 단어가 많이 사용되는데, 두 단어가 헷갈리기 쉬우므로 정리하고 넘어가자

<br/>

- `Cute DolphaGo` 라는 문자열이 분석기를 거쳐 인덱스에 저장된다고 가정
- 분석기는 먼저 캐릭터 필터를 통해 원문에서 불필요한 문자를 제거함
  - 이 과정까진 문자열 자체가 분리되는 것은 아니다. 그냥 필터링된 문자열이라고 보자.
- 이후 분석기는 **토크나이저**를 통해서 필터링된 문자열을 자르게 되는데, 이때 잘린 단위를 **토큰**이라고 지칭한다.
- 이러한 토큰들은 복수의 토큰 필터를 거치며 정제되는데, 정제 후 최종으로 역인덱스에 저장되는 상태의 토큰들을 **용어**라고 한다.
- 정리하자면, **토큰은 분석기 내부에서 일시적으로 존재하는 상태**이고, **인덱싱 되어 있는 단위, 또 검색에 사용되는 단위는 모두 용어**라고 이해하면 된다.
- 즉, [`Cute`, `DolphaGo`]는 토큰이고, 토큰 필터를 통해 Cute -> cute, DolphaGo -> dolphago 로 변환되어서 [`cute`, `dolphago`]는 용어라고 이해하면 되겠다.

## 분석기 구성

- 분석기에는 3가지 구성요소가 있다.

| 구성요소        | 설명                                                                              |
| --------------- | --------------------------------------------------------------------------------- |
| **캐릭터 필터** | 입력받은 문자열을 변경하거나 불필요한 문자들을 제거함                             |
| **토크나이저**  | 문자열을 토큰으로 분리. 분리할 때 토큰의 순서나 시작, 끝 위치도 기록함            |
| **토큰 필터**   | 분리된 토큰들의 필터 작업을 한다. **대소문자 구분, 형태소 분석 등의 작업이 가능** |

- 캐릭터 필터 : 문자열의 전처리 작업 담당
- 토크나이저 : 문자열을 토큰으로 분리
  - [가위 바위 보] -> [가위, 바위, 보]
- 토큰 필터 : 토크나이저에 분리된 토큰들에 대한 후처리 작업
- 분리된 토큰(가위, 바위, 보)은 필터를 거쳐서 최종적으로 용어가 됨
- 용어를 인덱싱해서 전문 검색에 활용할 수 있게 되는 것임
- 예를 들면 `가위`라는 용어로 [가위 바위 보]를 찾는 것임


### 역인덱싱
- 역인덱싱이란, 분석기가 문자열을 토큰화하고 이를 인덱싱하는 것을 말함


> 예제
문서 1: "I love Cute DolphaGo"
문서 2 : "The DolphaGo is Cute. Sorry. this is just example."

위 문서들은 분석기를 거쳐서 역인덱스 테이블에 다음과 같이 저장된다.
| 번호 | 단어     | 문서 |
| ---- | -------- | ---- |
| 1    | cute     | 1, 2 |
| 2    | dolphago | 1, 2 |
| 3    | example  | 2    |
| 4    | i        | 1    |
| 5    | just     | 2    |
| 6    | love     | 1    |
| 7    | sorry    | 2    |

- 2개의 문서가 분석기를 거치면서 역 인덱싱 되는 과정
- 분석기는 여러 필터와 토크나이저로 이뤄져 있는데 여기서는 스탠다드 토크나이저와 스톱필터, 소문자 변경 필터, 스테머 필터가 조합된 분석기를 사용한 것
- **역인덱스 테이블**은 용어가 어떤 문서에 속해 있는지 기록되어 있어서 손쉽게 문서를 찾을 수 있다.

### 분석기 API

- 엘라스틱서치는 필터와 토크나이저를 테스트할 수 있는 analyze라는 이름의 REST API를 제공함
- 자세한건 [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html)를 보자

> 스톱 분석기 테스트
- 위의 예시였던 첫번째 문장을 분석해보자.
```
POST _analyze
{
  "analyzer": "stop",
  "text": "I love Cute DolphaGo"
}
```
-> Response
```
{
  "tokens" : [
    {
      "token" : "i",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "love",
      "start_offset" : 2,
      "end_offset" : 6,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "cute",
      "start_offset" : 7,
      "end_offset" : 11,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "dolphago",
      "start_offset" : 12,
      "end_offset" : 20,
      "type" : "word",
      "position" : 3
    }
  ]
}
```
- 위의 예시였던 두번째 문장을 분석해보자.
```
POST _analyze
{
  "analyzer": "stop",
  "text": "The DolphaGo is Cute. Sorry. this is just example."
}
```
-> Response
```
{
  "tokens" : [
    {
      "token" : "dolphago",
      "start_offset" : 4,
      "end_offset" : 12,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "cute",
      "start_offset" : 16,
      "end_offset" : 20,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "sorry",
      "start_offset" : 22,
      "end_offset" : 27,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "just",
      "start_offset" : 37,
      "end_offset" : 41,
      "type" : "word",
      "position" : 7
    },
    {
      "token" : "example",
      "start_offset" : 42,
      "end_offset" : 49,
      "type" : "word",
      "position" : 8
    }
  ]
}
```

- 위와 같이 analyze api 사용법은 간단함
- analyzer에 원하는 분석기 선택 -> text에 분석할 문자열 입력
- 참고로 stop 분석기는 소문자 변경(lowercase) 토크나이저와 스톱(stop) 토큰 필터로 구성되어 있다.
- `this`나 `is` 같은건 토큰으로 지정되지 않았다는 것도 확인할 수 있다.
  - 스톱 분석기는 불용어(데이터 집합에 출연하는 빈도는 매우 높지만 의미는 딱히 없는 단어)를 처리한다.

### 분석기 종류

- [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html)

- "The DolphaGo is Cute. Sorry. this is just example." 라는 문장을 자주 사용되는 분석기와 함께 확인해보자.

> 자주 사용되는 분석기

| 분석기     | 설명                                                                                                                                                       | 결과                                                                           |
| ---------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| standard   | 특별한 설정이 없으면 엘라스틱 서치가 기본적으로 사용하는 분석기다. 영문법을 기준으로 한 스탠다드 토크나이저와 소문자 변경 필터, 스톱 필터가 포함되어 있다. | [ `this`, `dolphago`, `is`, `cute`, `sorry`, `this`, `is`, `just`, `example`]  |
| simple     | 문자만 토큰화한다. 공백, 숫자, 하이픈(-)이나 작은 따음표(') 같은 문자는 토큰화 하지 않는다.                                                                | [ `this`, `dolphago`, `is`, `cute`, `sorry`, `this`, `is`, `just`, `example`]  |
| whitespace | 공백을 기준으로 구분하여 토큰화한다.                                                                                                                       | [`The`, `DolphaGo`, `is`, `Cute.`, `Sorry.`, `this`, `is`, `just`, `example.`] |
| stop       | simple 분석기와 비슷하지만 **스톱 필터가 포함되었다.** 스톱 필터에 의해 의미가 없는 단어는 토큰화하지 않는다.                                              | [`dolphago`, `cute`, `sorry`, `just`, `example`]                               |


- 이게 각 분석기마다 갖고 있는 토크나이저나 필터들의 조합이 달라서, 만족하는 조합이 없으면 직접 만들어서 사용하는 것도 방법이다.