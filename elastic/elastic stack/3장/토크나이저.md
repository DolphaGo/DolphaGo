# 토크나이저

- [토크나이저](#토크나이저)
  - [자주 사용되는 토크나이저 종류](#자주-사용되는-토크나이저-종류)
    - [standard](#standard)
    - [lowercase](#lowercase)
    - [ngram](#ngram)
    - [uax_url_email](#uax_url_email)
  
- 분석기는 반드시 하나의 토크나이저를 포함해야 한다.
- **토크나이저는 문자열을 분리해서 토큰화하는 역할**이다.
- 분석기에 반드시 쓰이는 녀석이기 때문에, 형태에 맞는 적절한 토크나이저를 선택해야 한다.
- 자주 사용하는 토크나이저를 몇개 보자.

## 자주 사용되는 토크나이저 종류

### standard
- 스탠다드 분석기가 사용하는 토크나이저
- 특별한 설정이 없으면 기본 토크나이저로 사용
- **쉼표(,)나 점(.) 같은 기호를 제거하며 텍스트 기반으로 토큰화함**

> 테스트

```
POST _analyze
{
  "tokenizer": "standard",
  "text": "DolphaGo is AlphaGo"
}
```
-> Response
```
{
  "tokens" : [
    {
      "token" : "DolphaGo",
      "start_offset" : 0,
      "end_offset" : 8,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "is",
      "start_offset" : 9,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "AlphaGo",
      "start_offset" : 12,
      "end_offset" : 19,
      "type" : "<ALPHANUM>",
      "position" : 2
    }
  ]
}
```

### lowercase
- 텍스트 기반으로 토큰화하며, **모든 문자를 소문자로 변경해 토큰화함**

> 테스트
```
POST _analyze
{
  "tokenizer": "lowercase",
  "text": "DolphaGo is AlphaGo"
}
```
-> Response
```
{
  "tokens" : [
    {
      "token" : "dolphago",
      "start_offset" : 0,
      "end_offset" : 8,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "is",
      "start_offset" : 9,
      "end_offset" : 11,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "alphago",
      "start_offset" : 12,
      "end_offset" : 19,
      "type" : "word",
      "position" : 2
    }
  ]
}
```

### ngram
- 원문으로부터 N개의 연속된 글자 단위를 모두 토큰화함
- 예를 들어 `엘라스틱서치`라는 원문을 `2gram`으로 토큰화하면 다음과 같다.
  - [엘라, 라스, 스틱, 틱서, 서치]와 같이 연속된 두글자를 모두 추출한다.
- 장점 : 원문으로부터 검색할 수 있는 거의 모든 조합을 얻어낼 수 있기에 정밀 부분 검색 가능
- 단점 : 토크나이징을 수행한 N개 이하의 글자수로는 검색이 불가능, 모든 조합을 추출하기에 저장 공간을 많이 차지함
- [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html)

> 테스트

- ngram은 디폴트로 min_gram : 1, max_gram : 2다.
```
POST _analyze
{
  "tokenizer": "ngram",
  "text": "엘라스틱서치"
}
```
-> Response
```
{
  "tokens" : [
    {
      "token" : "엘",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "엘라",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "라",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "라스",
      "start_offset" : 1,
      "end_offset" : 3,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "스",
      "start_offset" : 2,
      "end_offset" : 3,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "스틱",
      "start_offset" : 2,
      "end_offset" : 4,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "틱",
      "start_offset" : 3,
      "end_offset" : 4,
      "type" : "word",
      "position" : 6
    },
    {
      "token" : "틱서",
      "start_offset" : 3,
      "end_offset" : 5,
      "type" : "word",
      "position" : 7
    },
    {
      "token" : "서",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "word",
      "position" : 8
    },
    {
      "token" : "서치",
      "start_offset" : 4,
      "end_offset" : 6,
      "type" : "word",
      "position" : 9
    },
    {
      "token" : "치",
      "start_offset" : 5,
      "end_offset" : 6,
      "type" : "word",
      "position" : 10
    }
  ]
}
```

- 만약 2글자로만 짤라내고 싶다면? 다음과 같이 커스터마이징을 해보자.
```
PUT dolphago-index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "dolphago_analyzer": {
          "tokenizer": "dolphago_tokenizer"
        }
      },
      "tokenizer": {
        "dolphago_tokenizer": {
          "type": "ngram",
          "min_gram": 3,
          "max_gram": 3,
          "token_chars": [
            "letter",
            "digit"
          ]
        }
      }
    }
  }
}

POST dolphago-index/_analyze
{
  "analyzer": "dolphago_analyzer",
  "text": "DolphaGo is man. He is 30 years old."
}
```
-> Response
```
{
  "tokens" : [
    {
      "token" : "Dol",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "olp",
      "start_offset" : 1,
      "end_offset" : 4,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "lph",
      "start_offset" : 2,
      "end_offset" : 5,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "pha",
      "start_offset" : 3,
      "end_offset" : 6,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "haG",
      "start_offset" : 4,
      "end_offset" : 7,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "aGo",
      "start_offset" : 5,
      "end_offset" : 8,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "man",
      "start_offset" : 12,
      "end_offset" : 15,
      "type" : "word",
      "position" : 6
    },
    {
      "token" : "yea",
      "start_offset" : 26,
      "end_offset" : 29,
      "type" : "word",
      "position" : 7
    },
    {
      "token" : "ear",
      "start_offset" : 27,
      "end_offset" : 30,
      "type" : "word",
      "position" : 8
    },
    {
      "token" : "ars",
      "start_offset" : 28,
      "end_offset" : 31,
      "type" : "word",
      "position" : 9
    },
    {
      "token" : "old",
      "start_offset" : 32,
      "end_offset" : 35,
      "type" : "word",
      "position" : 10
    }
  ]
}
```




### uax_url_email
- 스탠다드 분석기와 비슷하나, URL이나 이메일을 토큰화하는데 강점이 있음

```
POST _analyze
{
  "tokenizer": "uax_url_email",
  "text": "email: dev.dolphago@gmail.com"
}
```
-> Response
```
{
  "tokens" : [
    {
      "token" : "email",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "dev.dolphago@gmail.com",
      "start_offset" : 7,
      "end_offset" : 29,
      "type" : "<EMAIL>",
      "position" : 1
    }
  ]
}
```
- 이메일이나 URL을 형태에 맞게 토큰화하는데 특화되어 있다.