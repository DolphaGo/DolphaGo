# 문제 상황 파악

- 일부 문서만 색인이 되고, 일부 문서는 색인 시 에러가 발생해요. (opensearch 에서 발생한 현상)

![](/images/2024-06-02-17-33-22.png)

aws opensearch 이기 때문에 cloudWatch 로 문제 상황을 파악해봅니다.

![](/images/2024-06-02-17-33-59.png)

확인을 해보니, Rejected thread가 증가합니다.

# 원인 분석

`ThreadpoolWriteRejected`

- 특정한 용량 이상으로 들어오면, 스레드가 꽉차서 Reject를 해버리는 현상을 의미합니다.

- 이 문서는 Rejected 가 되면서 문서 색인이 안됐던 것이다.

그런데 더 자세히 확인해보니, 클러스터 전체가 아니라, 특정 노드에 대해서만 에러가 발생하고 있던 것입니다.

![](/images/2024-06-02-17-35-28.png)

여러 대의 노드 중에서 한 대만 유독 ThreadpoolWriteRejected가 발생합니다.

# 문제 해결

해당 노드를 클러스터에서 제거합니다.

opensearch 이기 때문에 id 기준으로 해당 노드를 클러스터에서 빼버리도록 합니다.

![](/images/2024-06-02-17-36-48.png)

우선 노드를 클러스터에서 제외 시키고, **샤드를 다른 노드들로 이동 시킨 후에 원인을 분석하거나 다른 노드로 교체**합니다.

ES에서 직접 서버를 올리고 했다면 확인해보는 과정이 있었겠지만, AWS Opensearch였기 때문에 노드에 대한 직접적인 권한이 없었습니다.

# 분석을 어렵게 만들었던 이유

첫 번째, OpenSearch 의 보안 패치 작업 (비슷한 시간 대에 작업이 있었어서)
두 번째, 문서의 색인이 일부는 성공하고 있던 점
- 하루 전체에 대해 확인해서 알 수 있었음. 5분, 30분 모니터링 했을 땐 몰랐을 것


결국.. 디테일한 모니터링이 중요하다.

> 정리

- 클러스터 내에 존재하는 다수의 노드들 중 하나의 노드에서 발생한 이상 동작이 전체 클러스터에 영향을 미친 경우 입니다.
- 클러스터 관점에서 모니터링도 중요하지만, 주요 지표들은 각 노드별로 설정하는 것도 필요합니다. (rejected)
- 모든 노드에서 WritepoolThreadRejected가 발생한 것과 특정 노드에서만 WritepoolThreadRejected가 발생한 것은 조치 방법도 원인도 다르기 때문입니다.